# 검색 시스템 가이드

Neo4j에 저장된 뉴스 기사를 GraphRAG 파이프라인으로 검색합니다.

## 개요

`notebooks/03_ToolsRetriever.ipynb`는 3종 Retriever를 구성하고, LLM이 질문에 따라 적절한 검색 도구를 자동 선택하는 **ToolsRetriever** 기반 GraphRAG를 구축합니다.

## 구성 요소

| 구분 | 모델 | 역할 |
|------|------|------|
| LLM | `qwen3:8b-q4_K_M` | 도구 선택 + 답변 생성 |
| Embedding | `bona/bge-m3-korean` (1024차원) | Content 노드 벡터 임베딩 |
| Graph DB | Neo4j | 그래프 데이터 저장/검색 |

## 3종 Retriever

### 1. VectorRetriever

Content 노드의 임베딩 벡터를 기반으로 의미 유사도 검색을 수행합니다.

- **용도**: 특정 주제, 인물, 사건에 대한 기사 검색
- **예시**: "삼성전자 실적 관련 기사", "트럼프 관세 정책"

### 2. VectorCypherRetriever

벡터 검색 결과에 Cypher 쿼리를 추가하여 관련 기사의 상세 정보와 같은 카테고리의 다른 기사를 함께 반환합니다.

- **용도**: 주제 검색 + 관련 기사 함께 필요할 때
- **예시**: "AI 관련 기사와 같은 카테고리의 다른 기사"

### 3. Text2CypherRetriever

자연어 질문을 Cypher 쿼리로 변환하여 구조적 검색을 수행합니다.

- **용도**: 카테고리별 기사 목록, 언론사별 기사 수, 최신 기사 N개 등
- **예시**: "정치 카테고리의 최신 뉴스", "카테고리별 기사 개수"

## ToolsRetriever

LLM이 질문을 분석하여 3종 Retriever 중 가장 적합한 도구를 자동 선택합니다.

```
사용자 질문
    │
    └─ LLM이 Tool 선택
         ├─ "삼성전자 관련 기사" → VectorRetriever
         ├─ "정치 카테고리 최신 뉴스" → Text2CypherRetriever
         └─ "AI 기사 + 관련 기사" → VectorCypherRetriever
    │
    └─ 선택된 Retriever가 검색 수행
    │
    └─ 검색 결과 + 프롬프트 → LLM → 최종 답변
```

## 임베딩 생성

STEP 2에서 생성된 Content 노드에 벡터 임베딩을 추가합니다. 노트북의 임베딩 섹션을 실행하면:

1. 모든 Content 노드의 `chunk` 텍스트를 임베딩 모델에 전달
2. 1024차원 벡터를 `embedding` 속성으로 저장
3. `content_vector_index` 벡터 인덱스 생성

임베딩은 최초 1회만 실행하면 됩니다. 데이터가 변경되지 않는 한 다시 실행할 필요가 없습니다.

## 실행 방법

1. STEP 2(GraphBuilder)까지 완료되어야 합니다.
2. Neo4j가 실행 중이고 Ollama가 동작해야 합니다.
3. `notebooks/03_ToolsRetriever.ipynb`를 순서대로 실행합니다.

### 노트북 실행 순서

| 셀 | 설명 |
|-----|------|
| 환경 설정 | 모듈 로드, .env 설정 |
| LLM/Embedding 초기화 | OllamaLLM, OllamaEmbeddings 인스턴스 생성 |
| 임베딩 생성 | Content 노드에 벡터 임베딩 추가 (최초 1회) |
| Retriever 구성 | Vector, VectorCypher, Text2Cypher 생성 |
| 개별 테스트 | 각 Retriever 독립 테스트 |
| ToolsRetriever | 3종 통합 + GraphRAG 파이프라인 |
| 질의 테스트 | `ask()` 함수로 질의응답 |

## LLM 파라미터

```python
model_params={
    "options": {
        "temperature": 0.1,
        "num_predict": 4096,
        "num_ctx": 8192,
        "repeat_penalty": 1.2,
        "repeat_last_n": 128,
    },
    "think": False,
}
```

| 파라미터 | 값 | 설명 |
|---------|-----|------|
| temperature | 0.1 | 낮을수록 일관된 답변 |
| num_predict | 4096 | 최대 생성 토큰 수 |
| num_ctx | 8192 | 컨텍스트 윈도우 크기 |
| repeat_penalty | 1.2 | 반복 억제 (너무 높으면 답변 깨짐) |
| think | False | Qwen3 추론 모드 비활성화 (속도 향상) |

## 다음 단계

검색 시스템이 동작하면 [웹 시각화 가이드](05_웹시각화.md)로 진행하세요.
