{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Neo4j 그래프 DB 구축 — Recursive Chunking 버전\n",
        "\n",
        "GraphBuilder.ipynb와 동일한 흐름이지만 **recursive_chunk_text**로 청킹하여 DB에 적재합니다.\n",
        "\n",
        "## GraphBuilder와의 차이\n",
        "\n",
        "| 항목 | GraphBuilder | 이 노트북 |\n",
        "|------|--------------|----------|\n",
        "| 청킹 함수 | `chunk_text` (고정 500자) | `recursive_chunk_text` |\n",
        "| 구분자 | 없음 (글자 수만) | `\\n\\n` → `\\n` → `. ` → ` ` 순 |\n",
        "| 비교 섹션 | 없음 | 있음 (고정 vs Recursive) |\n",
        "\n",
        "## 구분자 우선순위 (RecursiveCharacterTextSplitter 스타일)\n",
        "\n",
        "1. `\\n\\n` (문단)\n",
        "2. `\\n` (줄)\n",
        "3. `. ` (문장 끝)\n",
        "4. `。` (한국어 마침표)\n",
        "5. `, ` (쉼표)\n",
        "6. ` ` (공백)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. 데이터 로드\n",
        "\n",
        "`data_scrapping.py`로 수집한 뉴스 기사 Excel 파일을 로드합니다. 최신 `Articles_*.xlsx` 자동 탐색."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "files = sorted(glob.glob(os.path.join('..', 'Articles_*.xlsx')), reverse=True)\n",
        "if not files:\n",
        "    raise FileNotFoundError('Articles_*.xlsx 파일을 찾을 수 없습니다. data_scrapping.py를 먼저 실행하세요.')\n",
        "\n",
        "input_file = files[0]\n",
        "df = pd.read_excel(input_file)\n",
        "print(f'Loaded: {input_file}, {len(df)} articles')\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "08bfdd6b"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Neo4j 연결"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import neo4j\n",
        "import os\n",
        "import dotenv\n",
        "\n",
        "dotenv.load_dotenv(dotenv_path=os.path.join(os.path.dirname(os.path.abspath(\"\")), \".env\"), override=True)\n",
        "\n",
        "URI = os.getenv(\"NEO4J_URI\", \"neo4j://localhost:7687\")\n",
        "AUTH = (os.getenv(\"NEO4J_USERNAME\", \"neo4j\"), os.getenv(\"NEO4J_PASSWORD\", \"password\"))\n",
        "DB_NAME = os.getenv(\"NEO4J_DB\", \"neo4j\")\n",
        "\n",
        "driver = neo4j.GraphDatabase.driver(URI, auth=AUTH)\n",
        "driver.verify_connectivity()\n",
        "print(f'Connected to {URI}')\n",
        "print(f'Database: {DB_NAME}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. 청킹 함수 정의\n",
        "\n",
        "- **chunk_text**: 고정 500자 + overlap 100 (비교용)\n",
        "- **recursive_chunk_text**: 구분자 우선순위 기반 Recursive 방식 (LangChain 없이 순수 Python)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def chunk_text(text, chunk_size=500, overlap=100):\n",
        "    \"\"\"고정 크기 청킹 (GraphBuilder.ipynb와 동일)\"\"\"\n",
        "    if pd.isna(text) or text == '':\n",
        "        return []\n",
        "    text = str(text)\n",
        "    chunks = []\n",
        "    for i in range(0, len(text), chunk_size - overlap):\n",
        "        chunk = text[i:i + chunk_size]\n",
        "        if chunk.strip():\n",
        "            chunks.append(chunk.strip())\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def recursive_chunk_text(text, chunk_size=500, overlap=100, separators=None):\n",
        "    \"\"\"\n",
        "    RecursiveCharacterTextSplitter 스타일 청킹 (LangChain 없이 순수 Python).\n",
        "    구분자 우선순위대로 분할하여 의미 경계를 존중합니다.\n",
        "    \"\"\"\n",
        "    if separators is None:\n",
        "        separators = [\"\\n\\n\", \"\\n\", \". \", \"。\", \", \", \" \"]\n",
        "    if not text or not str(text).strip():\n",
        "        return []\n",
        "    text = str(text).strip()\n",
        "\n",
        "    def _split(t, sep):\n",
        "        if sep == \"\":\n",
        "            return list(t)\n",
        "        return t.split(sep)\n",
        "\n",
        "    def _merge_splits(splits, sep):\n",
        "        if not splits:\n",
        "            return []\n",
        "        if sep:\n",
        "            splits = [s + sep for s in splits[:-1]] + [splits[-1]]\n",
        "        chunks = []\n",
        "        current = []\n",
        "        for s in splits:\n",
        "            add_len = len(s)\n",
        "            merged_so_far = \"\".join(current)\n",
        "            if len(merged_so_far) + add_len <= chunk_size:\n",
        "                current.append(s)\n",
        "            else:\n",
        "                if current:\n",
        "                    merged = \"\".join(current)\n",
        "                    if merged.strip():\n",
        "                        chunks.append(merged.strip())\n",
        "                if overlap and current:\n",
        "                    prev = \"\".join(current)\n",
        "                    tail = prev[-overlap:] if len(prev) >= overlap else prev\n",
        "                    current = [tail, s] if tail.strip() else [s]\n",
        "                else:\n",
        "                    current = [s]\n",
        "        if current:\n",
        "            merged = \"\".join(current)\n",
        "            if merged.strip():\n",
        "                chunks.append(merged.strip())\n",
        "        return chunks\n",
        "\n",
        "    def _split_recursive(t, sep_idx):\n",
        "        if not t or not t.strip():\n",
        "            return []\n",
        "        if sep_idx >= len(separators):\n",
        "            if len(t) <= chunk_size:\n",
        "                return [t] if t.strip() else []\n",
        "            return [t[:chunk_size]] + _split_recursive(t[chunk_size - overlap:], sep_idx)\n",
        "        sep = separators[sep_idx]\n",
        "        splits = _split(t, sep)\n",
        "        merged = _merge_splits(splits, sep if sep != \"\" else None)\n",
        "        result = []\n",
        "        for m in merged:\n",
        "            if len(m) <= chunk_size:\n",
        "                result.append(m)\n",
        "            else:\n",
        "                result.extend(_split_recursive(m, sep_idx + 1))\n",
        "        return result\n",
        "\n",
        "    return _split_recursive(text, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. 청킹 비교 (고정 vs Recursive)\n",
        "\n",
        "기사 1~2건에 대해 두 방식의 결과를 비교합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for idx in range(min(2, len(df))):\n",
        "    row = df.iloc[idx]\n",
        "    text = row.get('content', '')\n",
        "    title = row.get('title', '')[:50]\n",
        "    if pd.isna(text) or not str(text).strip():\n",
        "        continue\n",
        "    \n",
        "    fixed = chunk_text(text, 500, 100)\n",
        "    recursive = recursive_chunk_text(text, 500, 100)\n",
        "    \n",
        "    print(f'=== 기사 {idx+1}: {title}... ===')\n",
        "    print(f'원문: {len(str(text))}자')\n",
        "    print(f'고정: {len(fixed)}개 청크')\n",
        "    for i, c in enumerate(fixed):\n",
        "        print(f'  [{i}] {len(c)}자 | {c[:60]}...')\n",
        "    print(f'Recursive: {len(recursive)}개 청크')\n",
        "    for i, c in enumerate(recursive):\n",
        "        print(f'  [{i}] {len(c)}자 | {c[:60]}...')\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. DB 초기화 및 제약조건"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def clear_database(tx):\n",
        "    tx.run(\"MATCH (n) DETACH DELETE n\")\n",
        "\n",
        "def create_constraints(tx):\n",
        "    constraints = [\n",
        "        \"CREATE CONSTRAINT IF NOT EXISTS FOR (a:Article) REQUIRE a.article_id IS UNIQUE\",\n",
        "        \"CREATE CONSTRAINT IF NOT EXISTS FOR (c:Content) REQUIRE c.content_id IS UNIQUE\",\n",
        "        \"CREATE CONSTRAINT IF NOT EXISTS FOR (m:Media) REQUIRE m.name IS UNIQUE\",\n",
        "        \"CREATE CONSTRAINT IF NOT EXISTS FOR (cat:Category) REQUIRE cat.name IS UNIQUE\",\n",
        "        \"CREATE CONSTRAINT IF NOT EXISTS FOR (au:Author) REQUIRE au.name IS UNIQUE\",\n",
        "    ]\n",
        "    for c in constraints:\n",
        "        tx.run(c)\n",
        "\n",
        "with driver.session(database=DB_NAME) as session:\n",
        "    session.execute_write(clear_database)\n",
        "    session.execute_write(create_constraints)\n",
        "print('DB 초기화 완료')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. 노드 및 관계 생성 함수"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def create_article_node(tx, article_data):\n",
        "    tx.run(\"\"\"\n",
        "        MERGE (a:Article {article_id: $article_id})\n",
        "        SET a.title = $title, a.url = $url, a.published_date = $published_date\n",
        "    \"\"\", **article_data)\n",
        "\n",
        "def create_content_nodes(tx, article_id, content_chunks, article_data):\n",
        "    for i, chunk in enumerate(content_chunks):\n",
        "        content_id = f\"{article_id}_chunk_{i}\"\n",
        "        tx.run(\"\"\"\n",
        "            MERGE (c:Content {content_id: $content_id})\n",
        "            SET c.chunk = $chunk, c.article_id = $article_id, c.title = $title,\n",
        "                c.url = $url, c.published_date = $published_date, c.chunk_index = $chunk_index\n",
        "        \"\"\", content_id=content_id, chunk=chunk, article_id=article_id,\n",
        "               title=article_data['title'], url=article_data['url'],\n",
        "               published_date=article_data['published_date'], chunk_index=i)\n",
        "        tx.run(\"\"\"\n",
        "            MATCH (a:Article {article_id: $article_id})\n",
        "            MATCH (c:Content {content_id: $content_id})\n",
        "            MERGE (a)-[:HAS_CHUNK]->(c)\n",
        "        \"\"\", article_id=article_id, content_id=content_id)\n",
        "\n",
        "def create_media_node_and_relationship(tx, article_id, source):\n",
        "    if pd.isna(source) or source == '': return\n",
        "    tx.run(\"MERGE (m:Media {name: $source})\", source=source)\n",
        "    tx.run(\"\"\"MATCH (a:Article {article_id: $article_id}) MATCH (m:Media {name: $source})\n",
        "              MERGE (m)-[:PUBLISHED]->(a)\"\"\", article_id=article_id, source=source)\n",
        "\n",
        "def create_category_node_and_relationship(tx, article_id, category):\n",
        "    if pd.isna(category) or category == '': return\n",
        "    tx.run(\"MERGE (cat:Category {name: $category})\", category=category)\n",
        "    tx.run(\"\"\"MATCH (a:Article {article_id: $article_id}) MATCH (cat:Category {name: $category})\n",
        "              MERGE (a)-[:BELONGS_TO]->(cat)\"\"\", article_id=article_id, category=category)\n",
        "\n",
        "def create_author_node_and_relationship(tx, article_id, author):\n",
        "    if pd.isna(author) or author == '': return\n",
        "    tx.run(\"MERGE (au:Author {name: $author})\", author=author)\n",
        "    tx.run(\"\"\"MATCH (a:Article {article_id: $article_id}) MATCH (au:Author {name: $author})\n",
        "              MERGE (au)-[:WROTE]->(a)\"\"\", article_id=article_id, author=author)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. 그래프 빌드 (recursive_chunk_text 사용)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def build_graph_from_dataframe(df, chunk_size=500, overlap=100):\n",
        "    \"\"\"recursive_chunk_text로 Content 노드 생성\"\"\"\n",
        "    with driver.session(database=DB_NAME) as session:\n",
        "        for idx, row in df.iterrows():\n",
        "            try:\n",
        "                article_id = row.get('article_id', '')\n",
        "                article_data = {\n",
        "                    'article_id': article_id,\n",
        "                    'title': row.get('title', ''),\n",
        "                    'url': row.get('url', ''),\n",
        "                    'published_date': str(row.get('published_date', ''))\n",
        "                }\n",
        "                session.execute_write(create_article_node, article_data)\n",
        "\n",
        "                if 'content' in row and pd.notna(row['content']) and row['content'] != '':\n",
        "                    content_chunks = recursive_chunk_text(row['content'], chunk_size, overlap)\n",
        "                    if content_chunks:\n",
        "                        session.execute_write(create_content_nodes, article_id, content_chunks, article_data)\n",
        "\n",
        "                if 'source' in row:\n",
        "                    session.execute_write(create_media_node_and_relationship, article_id, row['source'])\n",
        "                if 'category' in row:\n",
        "                    session.execute_write(create_category_node_and_relationship, article_id, row['category'])\n",
        "                if 'author' in row:\n",
        "                    session.execute_write(create_author_node_and_relationship, article_id, row['author'])\n",
        "\n",
        "                if (idx + 1) % 10 == 0:\n",
        "                    print(f'진행률: {idx + 1}/{len(df)} ({((idx + 1)/len(df)*100):.1f}%)')\n",
        "            except Exception as e:\n",
        "                print(f'기사 {idx} 처리 중 오류: {e}')\n",
        "                continue\n",
        "\n",
        "build_graph_from_dataframe(df, chunk_size=500, overlap=100)\n",
        "print('\\nGraph build complete! (recursive_chunk_text)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. 검증 — 그래프 요약"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "with driver.session(database=DB_NAME) as session:\n",
        "    result = session.run(\"\"\"\n",
        "        MATCH (n) RETURN labels(n)[0] AS Label, count(n) AS Count\n",
        "        ORDER BY Count DESC\n",
        "    \"\"\")\n",
        "    print('=== Node Counts ===')\n",
        "    for r in result:\n",
        "        print(f'  {r[\"Label\"]:12s}: {r[\"Count\"]:>5d}')\n",
        "\n",
        "    result = session.run(\"\"\"\n",
        "        MATCH ()-[r]->() RETURN type(r) AS Relationship, count(r) AS Count\n",
        "        ORDER BY Count DESC\n",
        "    \"\"\")\n",
        "    print('\\n=== Relationship Counts ===')\n",
        "    for r in result:\n",
        "        print(f'  {r[\"Relationship\"]:12s}: {r[\"Count\"]:>5d}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "with driver.session(database=DB_NAME) as session:\n",
        "    result = session.run(\"\"\"\n",
        "        MATCH (a:Article)-[:BELONGS_TO]->(cat:Category)\n",
        "        RETURN cat.name AS Category, count(a) AS Articles ORDER BY Articles DESC\n",
        "    \"\"\")\n",
        "    print('=== Articles per Category ===')\n",
        "    for r in result:\n",
        "        print(f'  {r[\"Category\"]:12s}: {r[\"Articles\"]}개')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "with driver.session(database=DB_NAME) as session:\n",
        "    result = session.run(\"\"\"\n",
        "        MATCH (a:Article) WITH a LIMIT 1\n",
        "        OPTIONAL MATCH (m:Media)-[:PUBLISHED]->(a)\n",
        "        OPTIONAL MATCH (a)-[:BELONGS_TO]->(cat:Category)\n",
        "        OPTIONAL MATCH (a)-[:HAS_CHUNK]->(c:Content)\n",
        "        RETURN a.article_id AS id, a.title AS title, m.name AS media,\n",
        "               cat.name AS category, count(c) AS chunks\n",
        "    \"\"\")\n",
        "    for r in result:\n",
        "        print(f'Article:  {r[\"id\"]}')\n",
        "        print(f'Title:    {r[\"title\"]}')\n",
        "        print(f'Media:    {r[\"media\"]}')\n",
        "        print(f'Category: {r[\"category\"]}')\n",
        "        print(f'Chunks:   {r[\"chunks\"]}개')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. 정리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "driver.close()\n",
        "print('Neo4j connection closed.')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}